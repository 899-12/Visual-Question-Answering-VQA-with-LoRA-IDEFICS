{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cU_wjwwyYbK",
        "outputId": "384d1e69-ab76-400f-9b11-7f0c0e93c6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install -q datasets\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q bitsandbytes sentencepiece accelerate loralib\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import IdeficsForVisionText2Text, AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n"
      ],
      "metadata": {
        "id": "4OJXHITj4yRD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model and processor\n",
        "model_name = \"HuggingFaceM4/idefics-9b\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HjQjU8MV5HTz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"]\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = IdeficsForVisionText2Text.from_pretrained(model_name, quantization_config=quant_config, device_map=\"auto\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kOn6Yipv5NI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for inference\n",
        "def generate_caption(model, processor, prompt, max_tokens=50):\n",
        "    tokenizer = processor.tokenizer\n",
        "    eos_id = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
        "    banned_tokens = tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n",
        "\n",
        "    inputs = processor(prompt, return_tensors=\"pt\").to(device)\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        eos_token_id=[eos_id],\n",
        "        bad_words_ids=banned_tokens,\n",
        "        max_new_tokens=max_tokens,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "    print(output_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "mm-BGgKe7sqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing helper for images\n",
        "def ensure_rgb(image):\n",
        "    if image.mode == \"RGB\":\n",
        "        return image\n",
        "    rgba = image.convert(\"RGBA\")\n",
        "    background = Image.new(\"RGBA\", rgba.size, (255, 255, 255))\n",
        "    return Image.alpha_composite(background, rgba).convert(\"RGB\")\n",
        "\n",
        "# Dataset transformation\n",
        "def preprocess_batch(batch):\n",
        "    size = processor.image_processor.image_size\n",
        "    mean = processor.image_processor.image_mean\n",
        "    std = processor.image_processor.image_std\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        ensure_rgb,\n",
        "        transforms.RandomResizedCrop((size, size), scale=(0.9, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "    prompts = []\n",
        "    for img_url, caption in zip(batch[\"image_url\"], batch[\"caption\"]):\n",
        "        clean_caption = caption.split(\".\")[0]\n",
        "        prompts.append([img_url, f\"Describe this image: {clean_caption}</s>\"])\n",
        "\n",
        "    inputs = processor(prompts, transform=transform, return_tensors=\"pt\").to(device)\n",
        "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Load and prepare the dataset\n",
        "dataset = load_dataset(\"nlphuji/flickr30k\", split=\"train[:1%]\").train_test_split(test_size=0.1)\n",
        "train_set, val_set = dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "train_set.set_transform(preprocess_batch)\n",
        "val_set.set_transform(preprocess_batch)\n",
        "\n"
      ],
      "metadata": {
        "id": "G6iYwhEf73xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    bias=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "-9ua55uY7558"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"idefics-flickr30k-lora\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_steps=25,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    save_steps=25,\n",
        "    logging_steps=5,\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    label_names=[\"labels\"],\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9QiuDFlA7_lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Trainer and start fine-tuning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=val_set\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MTR0s7yd8BU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}